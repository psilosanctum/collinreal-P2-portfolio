{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Comma AI - Video Processing Driving Data\"\n",
        "author: 'Collin Real'\n",
        "execute: \n",
        "    warning: false\n",
        "    error: false\n",
        "format: html\n",
        "theme: cyborg\n",
        "---"
      ],
      "id": "1541850c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Comma.ai is one of the few respectable tech companies offering one of the most advanced self-driving products: **the comma 3x.**\n",
        "  - Automates ~70+% of daily driving.\n",
        "  - Performs exceptionally well on highways and other roads with identifiable lanes.\n",
        "  - Installed and mounted on a car's front windshield, so it can receive a live data feed of the road.\n",
        "  - Using this live feed, the comma 3x projects the path for the vehicle to follow.\n",
        "\n",
        "### Comma API\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Comma uploads driving data to its servers to train better models and improve the self-driving experience over time. \n",
        "We can access our driving data using the [comma API](https://api.comma.ai/#comma-api-spec). \n",
        "Using our driving data, we can create metrics to analyze our driving patterns and behavior.\n",
        "\n",
        "### My Comma 3x device\n",
        "\n",
        "Visit the website for a more comprehensive overview: [comma.ai](https://www.comma.ai/)\n",
        "\n",
        "## Set Up Virtual Environment/Install Dependencies (Mac)\n",
        "### Execute these commands in your terminal\n",
        "- **Create local virtual env:** `python3 -m venv .venv`\n",
        "- **Activate local virtual env:** `source .venv/bin/activate`\n",
        "- **Install Python dependencies:** `pip3 install -r requirements.txt`\n",
        "- **Install Homebrew:** `/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"`\n",
        "- **Install 'ffmpeg' via Homebrew:** `brew install ffmpeg`\n",
        "- **OPTIONAL - Connect your personal Comma AI device:**\n",
        "    - `touch .env`\n",
        "    - `nano .env` - opens .env file in terminal\n",
        "    - `COMMA_AI_KEY=\"insert your Comma API key\"`\n",
        "    - `DONGLE_ID=\"insert your dongle ID\"`\n",
        "    - Save file and exit nano\n",
        "\n",
        "\n",
        "## Import Libraries & Set Configurations"
      ],
      "id": "4689cc7e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd # data processing\n",
        "import urllib.request # download file from URL\n",
        "import ssl # bypass SSL certificate\n",
        "import warnings # ignore non-critical warning outputs\n",
        "import cv2 # video processing\n",
        "import matplotlib.pyplot as plt # data visualization\n",
        "import matplotlib.image as mpimg # data visualization\n",
        "import subprocess # running terminal commands in Python script\n",
        "import seaborn_image as isns # data visualization\n",
        "from requests import get # API request\n",
        "from time import sleep # Prevent triggering the API limit\n",
        "from os import environ, listdir, mkdir, makedirs # directory manipulation & file saving\n",
        "import os.path\n",
        "from dotenv import load_dotenv # load environment variables\n",
        "from tqdm import tqdm # added as a meme, prints unnecessary loading bar in terminal during for loops\n",
        "from moviepy.editor import VideoFileClip, concatenate_videoclips # video editing\n",
        "plt.style.use('ggplot')\n",
        "warnings.filterwarnings('ignore')\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "pd.set_option('display.max_columns', None)\n",
        "load_dotenv()"
      ],
      "id": "e8627c33",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Variables for API Requests\n",
        "The first step to receive the recording of my longest trip (College Station) since installation is making sure I send the correct parameters to the API endpoint. Comma.ai's API requests require an authentication token and the dongle ID of a user's Comma device."
      ],
      "id": "4bd48aaf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "results": "hide"
      },
      "source": [
        "TOKEN= environ.get('COMMA_AI_KEY')\n",
        "DONGLE_ID = environ.get('DONGLE_ID')\n",
        "headers = {\n",
        "    'Authorization': 'JWT {}'.format(TOKEN)\n",
        "}\n",
        "BASE_URL = 'https://api.commadotai.com'"
      ],
      "id": "b0099714",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create/Check File Paths Exist"
      ],
      "id": "f56c0f2b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "route_data_path = 'data/route-data'\n",
        "vid_urls_path = 'data/vid-urls'\n",
        "vid_save_files_path = 'data/vid-files'\n",
        "mp4_directory = 'data/vid-mp4'\n",
        "full_vid_path = 'data/vid-full'\n",
        "images_path = 'data/route-images'\n",
        "\n",
        "paths = [\n",
        "    route_data_path,\n",
        "    vid_urls_path,\n",
        "    vid_save_files_path,\n",
        "    mp4_directory,\n",
        "    full_vid_path,\n",
        "    images_path,\n",
        "]\n",
        "\n",
        "for route_vid_path in paths:\n",
        "    if os.path.exists(route_vid_path) == False: mkdir(route_vid_path)\n",
        "    else: pass"
      ],
      "id": "989599aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Request #1 - Returns User Driving Data\n",
        "After creating the API variables, we can request the API endpoint which returns our driving data in the response output. The first API request will return various metrics from all of my driving trips since installing my Comma 3x. It will provide us with the route name for every trip. For our current task, we have chosen our longest trip by miles, so we will sort the dataset by longest trip to identify the route name. After sorting by descending order, the first row's value in column **fullname** is the route name."
      ],
      "id": "32709221"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def query_route_data(BASE_URL: str):\n",
        "    # Send API request\n",
        "    resp = get(\n",
        "        f'{BASE_URL}/v1/devices/{DONGLE_ID}/routes_segments?start=1706050612200&end=1811678741855', headers=headers, \n",
        "        verify=False)\n",
        "\n",
        "    # Convert API response to JSON\n",
        "    content = resp.json()\n",
        "\n",
        "    # Create DataFrame w/ API Response\n",
        "    df = pd.DataFrame(content)\n",
        "\n",
        "    # Remove latitude, longitude variables for privacy.\n",
        "    df = df[[\n",
        "        'fullname', 'length', 'create_time', 'end_time_utc_millis',\n",
        "        'end_time', 'init_logmonotime', 'maxqcamera', 'maxqlog', \n",
        "        'platform', 'procqcamera', 'procqlog', 'segment_end_times', \n",
        "        'segment_numbers', 'segment_start_times', 'start_time_utc_millis', 'version'\n",
        "    ]]\n",
        "\n",
        "    # Time metric conversions\n",
        "    df['time_diff_millis'] = df['end_time_utc_millis'] - df['start_time_utc_millis']\n",
        "    df['time_diff_seconds'] = df['time_diff_millis'].__truediv__(1000)\n",
        "    df['time_diff_minutes'] = df['time_diff_seconds'].__truediv__(60)\n",
        "    df['time_diff_hours'] = df['time_diff_minutes'].__truediv__(60)\n",
        "    df['end_time'] = pd.to_datetime(df['end_time']).dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # strip_dongle_id\n",
        "    removed_dongle_route_list = []\n",
        "    for idx, row in df.iterrows():\n",
        "        stripped_value = row['fullname'].replace(f'{DONGLE_ID}', 'INSERT-DONGLE-ID-HERE')\n",
        "        removed_dongle_route_list.append(stripped_value)\n",
        "    df['fullname'] = removed_dongle_route_list\n",
        "    \n",
        "    # df = df.sort_values('end_time_utc_millis', ascending=False)\n",
        "    df = df.sort_values('length', ascending=False)\n",
        "    route_names = df['fullname'].tolist()\n",
        "    route_df = pd.DataFrame()\n",
        "    route_df['route_name'] = route_names\n",
        "\n",
        "    # Save route data to csv\n",
        "    route_df.to_csv(f'{route_data_path}/route_names.csv', index=False)\n",
        "    df.to_csv(f'{route_data_path}/trip_driving_data.csv', index=False)\n",
        "    print(df.head(5))\n",
        "\n",
        "query_route_data(BASE_URL=BASE_URL)"
      ],
      "id": "f73c068e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Request #2 - Returns URLs To Download Video Files\n",
        "Using the route name, we can submit our second API request to an endpoint storing the URLs of our downloadable video files (.ts file type). Before downloading our files, we store the URLs from the API response in a text file, so we can access the URL data locally. "
      ],
      "id": "96e5ee8a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def query_to_extract_urls(BASE_URL: str, route_name: str):\n",
        "    df = pd.read_csv(f'{route_data_path}/route_names.csv')\n",
        "\n",
        "    # Insert dongle ID into route name\n",
        "    route_name_dongle_list = []\n",
        "    for idx, row in df.iterrows():\n",
        "        converted_route_name = row['route_name'].replace(\n",
        "            'INSERT-DONGLE-ID-HERE', f'{DONGLE_ID}')\n",
        "        route_name_dongle_list.append(converted_route_name)\n",
        "    df['route_name'] = route_name_dongle_list\n",
        "\n",
        "    download_recent_trip_vids = df.loc[df['route_name'] == route_name]\n",
        "    download_recent_trip_vids = download_recent_trip_vids['route_name'].tolist()\n",
        "\n",
        "    for route in tqdm(download_recent_trip_vids):\n",
        "        with get(\n",
        "            f'{BASE_URL}/v1/route/{route}/files', \n",
        "            headers=headers, verify=False, \n",
        "            stream=True, \n",
        "            timeout=10) as response:\n",
        "            content = response.json()['qcameras']\n",
        "            with open(\n",
        "                f'{vid_urls_path}' + f'/{route.replace(f\"{DONGLE_ID}|\", \"\")}.txt',\n",
        "                mode=\"wb\") as file:\n",
        "                for url in content:\n",
        "                    file.write(\n",
        "                        url.replace(\n",
        "                            f\"{DONGLE_ID}\", \n",
        "                            \"INSERT-DONGLE-ID-HERE\").encode('utf-8') + ' \\n'.encode('utf-8'))\n",
        "    urls_list = []\n",
        "    with open(\n",
        "        f'{vid_urls_path}' + f'/{route.replace(f\"{DONGLE_ID}|\", \"\")}.txt',\n",
        "        mode=\"r\") as file:\n",
        "        url_list = file.readlines()\n",
        "        for url in url_list:\n",
        "            urls_list.append(url)\n",
        "    print(\"Total number of URLs to download:\", len(urls_list))\n",
        "    print(\"\\n Preview 5 URLs:\", *url_list[:5], sep='\\n')\n",
        "\n",
        "query_to_extract_urls(BASE_URL=BASE_URL, route_name=f'{DONGLE_ID}|2024-04-07--06-08-29')"
      ],
      "id": "63ee9023",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading Our Driving Video .ts Files\n",
        "With our URLs stored locally in a text file, we can iterate over and request each URL to download and save our video files locally. <br/>\n",
        "**Note:** You cannot run this function since I did not provide my API token or dongle id"
      ],
      "id": "71aaa034"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def download_vid_files_from_url():\n",
        "    for filename in tqdm(listdir(vid_urls_path)):\n",
        "        print(\"Video URLs file:\", vid_urls_path +  f'/{filename}')\n",
        "        count = 0\n",
        "        f = os.path.join(vid_urls_path, filename)\n",
        "        file = open(f, 'rb')\n",
        "        print(\"Beginning video downloads...\")\n",
        "        for url in tqdm(file):\n",
        "            decode_url = url.decode('utf-8')\n",
        "            url_insert_dongle_id = decode_url.replace(\n",
        "                \"INSERT-DONGLE-ID-HERE\", f\"{DONGLE_ID}\")\n",
        "            create_route_vid_path = filename.replace('.txt', '').replace(f'{DONGLE_ID}|', '')\n",
        "            urllib.request.urlretrieve(\n",
        "                url_insert_dongle_id, \n",
        "                vid_save_files_path +\n",
        "                f'/{create_route_vid_path}' + \n",
        "                f'/x{str(count).rjust(3, \"0\")}_' + \n",
        "                f'{filename.replace(\".txt\", \"\").replace(f\"{DONGLE_ID}|\", \"\")}.ts')\n",
        "            count += 1\n",
        "        sleep(17)\n",
        "        print(\"Video files successfully downloaded!\")\n",
        "        print(\"Total files downloaded:\", count)\n",
        "        \n",
        "download_vid_files_from_url()"
      ],
      "id": "a4cfb49d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting File Type to MP4\n",
        "After looping over the URLs to download our driving videos, we convert our video file type from .ts to .mp4 since it's one of the most common file types for videos. We store the converted videos in a separate directory, so that we can loop over the 147 files without the original files making trouble."
      ],
      "id": "f7bfb519"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def convert_ts_to_mp4(vid_clip_directory: str):\n",
        "    route_directory = vid_save_files_path + vid_clip_directory\n",
        "    if path.exists(mp4_directory + vid_clip_directory) == False: \n",
        "        mkdir(mp4_directory + vid_clip_directory)\n",
        "    else: pass\n",
        "    \n",
        "    files_list = []\n",
        "    for file in listdir(route_directory): files_list.append(file)\n",
        "    files_list.sort()\n",
        "    for filename in files_list:\n",
        "        infile = route_directory + f'/{filename}'\n",
        "        outfile = mp4_directory + f'/{vid_clip_directory}' + f'/{filename.replace(\".ts\", \"\")}.mp4'\n",
        "        subprocess.run([\n",
        "            'ffmpeg',\n",
        "            '-i',\n",
        "            infile,\n",
        "            outfile,\n",
        "        ])\n",
        "# convert_ts_to_mp4(vid_clip_directory='/2024-04-07--06-08-29')"
      ],
      "id": "dca901a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concatenate The Video Clips\n",
        "To facilitate the distribution of video data, Comma API splits our video data into short clips to reduce the memory size. Our objective is to capture images from our entire trip; therefore, we need to concatenate the 147 video files. Ideally, we'd prefer to create one MP4 from the concatenation. Due to storage size, we split the final trip into 4 parts. If we don't split the video data in this manner, the file size would be too large and we wouldn't be able to push the video to GitHub."
      ],
      "id": "521cc2e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def concat_vid_clips(vid_clip_directory: str):\n",
        "    vid_clips_list = []\n",
        "    route_mp4_path = mp4_directory + vid_clip_directory\n",
        "\n",
        "    files_list = []\n",
        "    for file in listdir(route_mp4_path): files_list.append(file)\n",
        "    files_list.sort()\n",
        "\n",
        "    def multi_part_full_vid(video_title: str, start_range: int, end_range: int):\n",
        "        for filename in files_list[start_range:end_range]:\n",
        "            f = os.path.join(route_mp4_path, filename)\n",
        "            vid_clip = VideoFileClip(f)\n",
        "            vid_clips_list.append(vid_clip)\n",
        "        final_clip = concatenate_videoclips(clips=vid_clips_list, method='chain')\n",
        "        final_clip.write_videofile(f'{full_vid_path}' + f'/{video_title}.mp4')\n",
        "        vid_clips_list.clear()\n",
        "\n",
        "    multi_part_full_vid(video_title=\"trip_part_1\", start_range=0, end_range=40)\n",
        "    multi_part_full_vid(video_title=\"trip_part_2\", start_range=41, end_range=80)\n",
        "    multi_part_full_vid(video_title=\"trip_part_3\", start_range=81, end_range=120)\n",
        "    multi_part_full_vid(video_title=\"trip_part_4\", start_range=121, end_range=147)\n",
        "\n",
        "# concat_vid_clips(vid_clip_directory='/2024-04-07--06-08-29')"
      ],
      "id": "af9624c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Images From The Video\n",
        "Finally, we play the videos and save an Image every 2500 frames."
      ],
      "id": "8937f0b8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def save_frame_range(\n",
        "    video_path: str, \n",
        "    start_frame: int, \n",
        "    stop_frame: int, \n",
        "    step_frame: int,\n",
        "    dir_path: str, \n",
        "    basename: str, \n",
        "    ext='png'):\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened(): return\n",
        "\n",
        "    makedirs(dir_path, exist_ok=True)\n",
        "    base_path = os.path.join(dir_path, basename)\n",
        "\n",
        "    digit = len(str(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))))\n",
        "\n",
        "    for n in range(start_frame, stop_frame, step_frame):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, n)\n",
        "        ret, frame = cap.read()\n",
        "        if ret: cv2.imwrite(f'{base_path}_{str(n).zfill(digit)}.{ext}', frame)\n",
        "        else: return\n",
        "\n",
        "save_frame_range(full_vid_path + '/trip_part_1.mp4', 0, 200000, \n",
        "                 2500, images_path, 'part1_video_img_frame')\n",
        "\n",
        "save_frame_range(full_vid_path + '/trip_part_2.mp4', 0, 200000, \n",
        "                2500, images_path, 'part2_video_img_frame')\n",
        "\n",
        "save_frame_range(full_vid_path + '/trip_part_3.mp4', 0, 200000, \n",
        "                 2500, images_path, 'part3_video_img_frame')\n",
        "\n",
        "save_frame_range(full_vid_path + '/trip_part_4.mp4', 0, 200000, \n",
        "                 2500, images_path, 'part4_video_img_frame')\n",
        "\n",
        "# image_dir = listdir(images_path)\n",
        "# for image in image_dir: print(image)         "
      ],
      "id": "0eb36a15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## College Station Trip: Part 1/4\n",
        "\n",
        "{{< video data/vid-full/trip_part_1.mp4 >}}\n",
        "\n",
        "\n",
        "\n",
        "## Notable Images From Part 1/4\n",
        "::: {layout-nrow=2}\n",
        "![College Station](data/route-images/part1_video_img_frame_002500.png)\n",
        "\n",
        "![Water Tower](data/route-images/part4_video_img_frame_25000.png)\n",
        "\n",
        "![Country Road](data/route-images/part3_video_img_frame_10000.png)\n",
        "\n",
        "![Traffic Light](data/route-images/part1_video_img_frame_010000.png)\n",
        ":::\n",
        "\n",
        "## Plotting Images "
      ],
      "id": "bed713da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cstat = f'{images_path}' + '/part1_video_img_frame_002500.png'\n",
        "water_tower = f'{images_path}' + '/part4_video_img_frame_25000.png'\n",
        "country_road = f'{images_path}' + '/part3_video_img_frame_10000.png'\n",
        "traffic_light = f'{images_path}' + '/part1_video_img_frame_010000.png'\n",
        "\n",
        "cstat_image = plt.imread(cstat, format='png')\n",
        "water_tower_image = plt.imread(water_tower, format='png')\n",
        "country_road_image = plt.imread(country_road, format='png')\n",
        "traffic_light_image = plt.imread(traffic_light, format='png')\n",
        "\n",
        "ax0 = isns.imgplot(cstat_image, cmap='seismic', gray=True)\n",
        "ax1 = isns.imgplot(water_tower_image, cmap='seismic', gray=True)\n",
        "ax2 = isns.imgplot(country_road_image, cmap='seismic', gray=True)\n",
        "ax3 = isns.imgplot(traffic_light_image, cmap='seismic', gray=True)\n",
        "plt.show()"
      ],
      "id": "b2362367",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting Images - Histograms"
      ],
      "id": "b71be36e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.subplot(2,2,1)\n",
        "plt.hist(cstat_image.ravel())\n",
        "plt.subplot(2,2,2)\n",
        "plt.hist(water_tower_image.ravel())\n",
        "plt.subplot(2,2,3)\n",
        "plt.hist(country_road_image.ravel())\n",
        "plt.subplot(2,2,4)\n",
        "plt.hist(traffic_light_image.ravel())\n",
        "plt.show()"
      ],
      "id": "f41efd4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting Images - Boxplots"
      ],
      "id": "68911da2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.subplot(2,2,1)\n",
        "plt.boxplot(cstat_image.ravel())\n",
        "plt.subplot(2,2,2)\n",
        "plt.boxplot(water_tower_image.ravel())\n",
        "plt.subplot(2,2,3)\n",
        "plt.boxplot(country_road_image.ravel())\n",
        "plt.subplot(2,2,4)\n",
        "plt.boxplot(traffic_light_image.ravel())\n",
        "plt.show()"
      ],
      "id": "6190d394",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}